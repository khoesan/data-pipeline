# ğŸ› ETL Pipeline Sederhana dari Web Scraping ke Repositori Data

Proyek ini membangun sebuah *data pipeline* sederhana yang dimulai dari proses **web scraping** data produk dari sebuah website, dilanjutkan dengan tahap pembersihan data, dan akhirnya memuat data ke dua repositori: file **CSV** (`products.csv`) dan **Google Sheets** menggunakan Google Sheets API.

Pipeline mengikuti tiga tahap utama dalam proses **ETL (Extract - Transform - Load)**. Proyek ini juga dilengkapi dengan **unit testing** serta **test coverage** untuk memastikan setiap komponen berjalan dengan baik.

---

## ğŸ“ Struktur Direktori

```
submission-pemda/
â”‚
â”œâ”€â”€ main.py                        # Script utama untuk menjalankan pipeline ETL
â”œâ”€â”€ products.csv                   # File output CSV hasil akhir data
â”œâ”€â”€ google-sheets-api.json        # Kredensial Google Sheets API (service account)
â”œâ”€â”€ requirements.txt              # Daftar dependensi Python
â”‚
â”œâ”€â”€ utils/                         # Modul ETL
â”‚   â”œâ”€â”€ extract.py                 # Scraping data dari website
â”‚   â”œâ”€â”€ transform.py              # Membersihkan & memformat data hasil scraping
â”‚   â””â”€â”€ load.py                   # Menyimpan data ke CSV & mengunggah ke Google Sheets
â”‚
â”œâ”€â”€ tests/                         # Unit test untuk masing-masing modul
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ test_load.py
â”‚
â”œâ”€â”€ submission.txt                 # Instruksi teknis (cara menjalankan skrip)
â””â”€â”€ .coverage                      # File hasil test coverage

```

---

## âš™ï¸ Instalasi

1. **Clone repo ini**:

   ```bash
   git clone <url-repo>
   cd submission-pemda
   ```

2. **Buat dan aktifkan virtual environment**:

   ```bash
   python -m venv venv
   source venv/bin/activate        # Linux/macOS
   venv\Scripts\activate           # Windows
   ```

3. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸš€ Menjalankan Pipeline

Jalankan pipeline ETL lengkap (Web Scraping â†’ Data Cleaning â†’ Load ke CSV & Google Sheets):

```bash
python main.py
```

Pastikan kredensial `google-sheets-api.json` tersedia di direktori root proyek.

---

## ğŸ§ª Menjalankan Unit Test

Proyek menggunakan `unittest` untuk pengujian fungsi:

```bash
python -m unittest discover tests
```

Untuk mengukur **test coverage**:

```bash
coverage run -m unittest discover tests
coverage report -m
```

---

## ğŸ”— Integrasi Google Sheets

Hasil data akhir akan diunggah secara otomatis ke Google Sheets melalui service account. Link spreadsheet tujuan:

[ğŸ”— Google Spreadsheet](https://docs.google.com/spreadsheets/d/14BctMkMRHxLgnYDMkaDFx9ccGVFI92pbeKbBu34kpCw/edit?gid=0)

---

## ğŸ›  Penjelasan Modul

* `extract.py`: Scrapes data dari website dan mengubahnya menjadi dataframe.
* `transform.py`: Membersihkan, memfilter, atau memformat data sesuai kebutuhan (misal: normalisasi nama kolom, konversi tipe data, penghapusan entri kosong).
* `load.py`: Menyimpan hasil transformasi ke `products.csv` dan mengunggahnya ke Google Sheets menggunakan `gspread`.

---

## ğŸ“ Catatan

* Untuk menggunakan service account Google Sheets, ikuti panduan resmi [di sini](https://docs.gspread.org/en/latest/oauth2.html#for-bots-using-service-account).
* Beberapa situs dapat memiliki mekanisme anti-scraping. Pastikan penggunaan data tetap sesuai dengan kebijakan mereka.

---

## ğŸ“„ Lisensi

Proyek ini dibuat untuk keperluan pembelajaran dan submission internal. Tidak untuk penggunaan komersial tanpa izin.
